{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1beebee3-e1ec-4742-a0d0-92a88af7446e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 1: Saving the visualisation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64919c2f-5a45-4be5-9892-02d56413146e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Installing required libraries for model and image processing\n",
    "!pip install transformers==4.44.2\n",
    "!pip install ultralytics\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install shap\n",
    "!pip install torch torchvision torchaudio \n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install bitsandbytes accelerate\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d0e0a2d-af67-4937-8eaa-be423523c08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import shap\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Load your custom-trained YOLOv8 object detection model\n",
    "model = YOLO(\"weights/yolov8lbest.pt\")\n",
    "\n",
    "# Define input directory with images and output directory for saving results\n",
    "input_dir = 'images/original'\n",
    "output_dir = \"images/output\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Class index to class name mapping\n",
    "class_name_mapping = {0: \"part_1\", 1: \"part_2\", 2: \"part_3\"}\n",
    "\n",
    "# YOLO prediction function for SHAP\n",
    "def yolo_predict(images):\n",
    "    images = [img for img in images]  # Ensure correct format for YOLOv8\n",
    "    results = model.predict(source=images, save=False, verbose=False)\n",
    "    \n",
    "    top_confidences, top_classes, bboxes = [], [], []\n",
    "    for result in results:\n",
    "        if len(result.boxes.data) > 0:\n",
    "            sorted_indices = torch.argsort(result.boxes.data[:, 4], descending=True)[:10]\n",
    "            top_10_confidences = result.boxes.data[sorted_indices][:, 4].cpu().numpy()\n",
    "            top_10_classes = result.boxes.data[sorted_indices][:, 5].cpu().numpy()\n",
    "            top_10_bboxes = result.boxes.data[sorted_indices][:, :4].cpu().numpy()\n",
    "            \n",
    "            if top_10_confidences.shape[0] < 10:\n",
    "                top_10_confidences = np.pad(top_10_confidences, (0, 10 - top_10_confidences.shape[0]), mode='constant')\n",
    "                top_10_classes = np.pad(top_10_classes, (0, 10 - top_10_classes.shape[0]), mode='constant')\n",
    "                top_10_bboxes = np.pad(top_10_bboxes, ((0, 10 - top_10_bboxes.shape[0]), (0, 0)), mode='constant')\n",
    "\n",
    "            top_confidences.append(top_10_confidences)\n",
    "            top_classes.append(top_10_classes)\n",
    "            bboxes.append(top_10_bboxes)\n",
    "        else:\n",
    "            top_confidences.append(np.zeros(10))\n",
    "            top_classes.append(np.zeros(10))\n",
    "            bboxes.append(np.zeros((10, 4)))\n",
    "\n",
    "    return np.array(top_confidences), np.array(top_classes), np.array(bboxes)\n",
    "\n",
    "# SHAP and bounding box saving function\n",
    "def save_shap_and_bbox(image_path, shap_values, image_batch, class_indices, confidences, bboxes, original_image):\n",
    "    num_outputs = shap_values[0].shape[-1]  # Number of outputs (for each predicted class/confidence)\n",
    "    highest_conf_per_class = {0: -1, 1: -1, 2: -1}\n",
    "    shap_to_save = {}\n",
    "    bbox_to_save = {}\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        class_idx = int(class_indices[i])\n",
    "        confidence = confidences[i]\n",
    "        \n",
    "        if confidence > highest_conf_per_class[class_idx]:\n",
    "            highest_conf_per_class[class_idx] = confidence\n",
    "            shap_to_save[class_idx] = i\n",
    "            bbox_to_save[class_idx] = bboxes[i]\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    for class_idx, i in shap_to_save.items():\n",
    "        class_name = class_name_mapping.get(class_idx, f\"Class {class_idx}\")\n",
    "        confidence = highest_conf_per_class[class_idx]\n",
    "        bbox = bbox_to_save[class_idx]\n",
    "\n",
    "        x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "        bbox_image = original_image.copy()\n",
    "        cv2.rectangle(bbox_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        cv2.putText(bbox_image, f\"{class_name} {confidence:.2f}\", (x_min, y_min - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        bbox_output_path = os.path.join(output_dir, f\"{base_filename}_{class_name}_bbox.png\")\n",
    "        cv2.imwrite(bbox_output_path, bbox_image)\n",
    "\n",
    "        plt.figure()\n",
    "        image_rgb_for_plotting = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        shap.image_plot([shap_values[..., i]], np.expand_dims(image_rgb_for_plotting, axis=0), show=False)\n",
    "        shap_output_path = os.path.join(output_dir, f\"{base_filename}_{class_name}_shap.png\")\n",
    "        plt.savefig(shap_output_path, bbox_inches='tight', dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "# SHAP explainer function\n",
    "def run_shap_explainer(image_path, image_rgb):\n",
    "    image_batch = np.expand_dims(image_rgb, axis=0)\n",
    "    confidences, classes, bboxes = yolo_predict([image_rgb])\n",
    "\n",
    "    masker_blur = shap.maskers.Image(\"blur(128,128)\", image_rgb.shape)\n",
    "    explainer = shap.Explainer(lambda x: yolo_predict(x)[0], masker_blur)\n",
    "    \n",
    "    shap_values = explainer(image_batch, max_evals=5000, batch_size=50)\n",
    "    save_shap_and_bbox(image_path, shap_values.values, image_batch, classes[0], confidences[0], bboxes[0], image_rgb)\n",
    "\n",
    "# DRISE function to generate saliency maps for each class\n",
    "def generate_drise_saliency_per_class(model, image_path, output_directory, label_names, n_masks=5000, grid_size=(16, 16), prob_thresh=0.2):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_h, image_w = image.shape[:2]\n",
    "\n",
    "    results = model(image)\n",
    "    preds = results[0].boxes.xyxy.cpu().numpy()\n",
    "    scores = results[0].boxes.conf.cpu().numpy()\n",
    "    pred_classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    best_boxes = {}\n",
    "    for i in range(len(preds)):\n",
    "        class_id = pred_classes[i]\n",
    "        confidence = scores[i]\n",
    "        bbox = preds[i]\n",
    "        if class_id not in best_boxes or confidence > best_boxes[class_id]['confidence']:\n",
    "            best_boxes[class_id] = {'bbox': bbox, 'confidence': confidence}\n",
    "\n",
    "    for class_id, info in best_boxes.items():\n",
    "        if class_id >= len(label_names):\n",
    "            logging.warning(f\"Class ID {class_id} exceeds the label names list length. Skipping.\")\n",
    "            continue\n",
    "        target_box = info['bbox']\n",
    "        confidence_score = info['confidence']\n",
    "        class_name = label_names[class_id]\n",
    "\n",
    "        saliency_map = np.zeros((image_h, image_w), dtype=np.float32)\n",
    "        for i in range(n_masks):\n",
    "            mask = generate_mask((image_w, image_h), grid_size, prob_thresh)\n",
    "            masked_image = mask_image(image, mask)\n",
    "            masked_results = model(masked_image)\n",
    "            masked_preds = masked_results[0].boxes.xyxy.cpu().numpy()\n",
    "            masked_scores = masked_results[0].boxes.conf.cpu().numpy()\n",
    "            masked_classes = masked_results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "            for box, score, cls in zip(masked_preds, masked_scores, masked_classes):\n",
    "                if cls == class_id:\n",
    "                    iou_score = iou(target_box, box)\n",
    "                    saliency_map += mask * iou_score * score\n",
    "\n",
    "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "        heatmap = cv2.applyColorMap((saliency_map * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        cam = cv2.addWeighted(image, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "        image_name = os.path.basename(image_path).split('.')[0]\n",
    "        output_path = os.path.join(output_directory, f\"{image_name}_{class_name}_saliency.png\")\n",
    "        cv2.imwrite(output_path, cam)\n",
    "        print(f\"Saved saliency map for class '{class_name}' at {output_path}\")\n",
    "\n",
    "# Supporting functions for DRISE\n",
    "def generate_mask(image_size, grid_size, prob_thresh):\n",
    "    image_w, image_h = image_size\n",
    "    grid_w, grid_h = grid_size\n",
    "    cell_w, cell_h = math.ceil(image_w / grid_w), math.ceil(image_h / grid_h)\n",
    "    up_w, up_h = (grid_w + 1) * cell_w, (grid_h + 1) * cell_h\n",
    "    mask = (np.random.uniform(0, 1, size=(grid_h, grid_w)) < prob_thresh).astype(np.float32)\n",
    "    mask = cv2.resize(mask, (up_w, up_h), interpolation=cv2.INTER_LINEAR)\n",
    "    offset_w = np.random.randint(0, cell_w)\n",
    "    offset_h = np.random.randint(0, cell_h)\n",
    "    return mask[offset_h:offset_h + image_h, offset_w:offset_w + image_w]\n",
    "\n",
    "def mask_image(image, mask):\n",
    "    return ((image.astype(np.float32) / 255 * np.dstack([mask] * 3)) * 255).astype(np.uint8)\n",
    "\n",
    "def iou(box1, box2):\n",
    "    box1 = np.asarray(box1)\n",
    "    box2 = np.asarray(box2)\n",
    "    tl = np.vstack([box1[:2], box2[:2]]).max(axis=0)\n",
    "    br = np.vstack([box1[2:], box2[2:]]).min(axis=0)\n",
    "    intersection = np.prod(br - tl) * np.all(tl < br).astype(float)\n",
    "    area1 = np.prod(box1[2:] - box1[:2])\n",
    "    area2 = np.prod(box2[2:] - box2[:2])\n",
    "    return intersection / (area1 + area2 - intersection)\n",
    "\n",
    "# Loop through each image in the directory and run both DRISE and SHAP explainers\n",
    "for image_name in os.listdir(input_dir):\n",
    "    image_path = os.path.join(input_dir, image_name)\n",
    "    if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.resize(image, (512, 512)) if image.shape[:2] != (512, 512) else image\n",
    "\n",
    "        # Run DRISE\n",
    "        generate_drise_saliency_per_class(model, image_path, output_dir, list(class_name_mapping.values()))\n",
    "        \n",
    "        # Run SHAP\n",
    "        run_shap_explainer(image_path, image_rgb)\n",
    "\n",
    "print(\"DRISE and SHAP operations completed for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c84c5d4-213c-4e5e-a8cd-59040fa64ddc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 2: Large Multimodal Model Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f7380a-75e6-48c8-a7fe-c1923555eb81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "# Function to clear memory and avoid out of memory issues\n",
    "def free_gpu_memory():\n",
    "    print(\"Clearing GPU memory...\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "DEVICE = \"cuda:0\" \n",
    "\n",
    "# BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # 4-bit quantization type\n",
    "    bnb_4bit_use_double_quant=True,    # Double quantization for memory efficiency\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Use FP16 for computation\n",
    ")\n",
    "\n",
    "# Load processor and deactivate image splitting for better memory efficiency\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    do_image_splitting=False,  # Deactivate image splitting\n",
    ")\n",
    "\n",
    "# Load model with FP16 and 4-bit quantization\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    torch_dtype=torch.float16,           # Use FP16 for reduced memory and faster computation\n",
    "    quantization_config=quantization_config,  # 4-bit quantization\n",
    ")  \n",
    "\n",
    "# Function to load images for a specific class ID\n",
    "def load_images_for_analysis(image_name, class_id, input_dir):\n",
    "    # Only three images of the same class ID\n",
    "    image_filenames = [\n",
    "        f\"{image_name}_{class_id}_bbox.png\",\n",
    "        f\"{image_name}_{class_id}_saliency.png\",\n",
    "        f\"{image_name}_{class_id}_shap.png\"\n",
    "    ]\n",
    "    \n",
    "    images = []\n",
    "    for filename in image_filenames:\n",
    "        image_path = os.path.join(input_dir, filename)\n",
    "        if os.path.exists(image_path):\n",
    "            images.append(load_image(image_path))\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image {filename} not found in {input_dir}.\")\n",
    "    return images, image_filenames  # Also return the filenames for display\n",
    "\n",
    "# Function to generate prompt for the model based on user input and image type\n",
    "def generate_prompt_for_image(image_name, class_id, question):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": f\"bounding_box_{class_id}\"},  # Bounding box image\n",
    "                {\"type\": \"image\", \"image\": f\"saliency_{class_id}\"},      # Saliency map\n",
    "                {\"type\": \"image\", \"image\": f\"shap_{class_id}\"},          # SHAP visualization\n",
    "                {\n",
    "                 \"type\": \"text\",\n",
    "                 \"text\": f\"Context: This task involves three visualizations that highlight various aspects of how the model interprets an object in an image.\\n\\n\"\n",
    "                         f\"1. **Bounding Box Visualization**: Shows the region where the object is detected, illustrating how the model identifies and localizes the object within the image.\\n\\n\"\n",
    "                         f\"2. **Saliency Map Visualization**: Highlights the areas of the image that are most important to the model's decision-making process, emphasizing the regions that influenced the prediction.\\n\\n\"\n",
    "                         f\"3. **SHAP (SHapley Additive exPlanations) Visualization**: Provides insights into how different parts of the image contribute to the modelâ€™s prediction, offering a clear understanding of the model's behavior.\\n\\n\"\n",
    "                         f\"Task: {question}\\n\\n\"\n",
    "                         f\"The goal of this analysis is to interpret and explain the provided visualizations, focusing on understanding how each image contributes to the overall prediction. Specifically, explain which areas of the image were most influential and where the object is located.\"\n",
    "                }\n",
    "\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Function to process images, ask questions, and conditionally display the images\n",
    "def ask_model(image_name, class_id, question, input_dir, display_images_flag=True):\n",
    "    try:\n",
    "        # Free memory before starting inference\n",
    "        free_gpu_memory()\n",
    "\n",
    "        # Load the 3 associated images and filenames from the input directory\n",
    "        images, image_filenames = load_images_for_analysis(image_name, class_id, input_dir)\n",
    "\n",
    "        # Generate prompt based on the user's question\n",
    "        prompt = generate_prompt_for_image(image_name, class_id, question)\n",
    "        \n",
    "        # Use processor to format the prompt and images for the model\n",
    "        with torch.no_grad():  # Prevent building computation graph during inference\n",
    "            inputs = processor.apply_chat_template(prompt, add_generation_prompt=True)\n",
    "            inputs = processor(text=inputs, images=images, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "            print(\"Processing model inference...\")  # Debug message\n",
    "\n",
    "            # Generate response from the model\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                top_k=10,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "            # Debug check to see if model output is captured\n",
    "            if generated_ids is None or len(generated_ids) == 0:\n",
    "                print(\"No output generated by the model.\")\n",
    "                return\n",
    "            \n",
    "            print(\"Model inference complete.\")  # Debug message\n",
    "\n",
    "            # Decode and return the generated response\n",
    "            generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Print the model's response\n",
    "            print(\"Model's Response:\")\n",
    "            print(generated_texts[0])\n",
    "            \n",
    "            # Conditionally display the images if the flag is set to True\n",
    "            if display_images_flag:\n",
    "                display_images(image_filenames, input_dir)\n",
    "\n",
    "            return generated_texts\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to display images using matplotlib\n",
    "def display_images(image_filenames, input_dir):\n",
    "    print(\"Displaying images...\")  # Debug message\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a row of 3 images\n",
    "    for i, filename in enumerate(image_filenames):\n",
    "        image_path = os.path.join(input_dir, filename)\n",
    "        image = Image.open(image_path)\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(filename.split('_')[-1].replace('.png', '').capitalize())  # Set the title as the type of image\n",
    "        axes[i].axis('off')  # Hide axis for better display\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93feae0-d254-44cb-b7d4-9b35c40949f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "image_name = \"12\"       # User provides an image name\n",
    "class_id = \"part_1\"    # Class ID (same for all three images)\n",
    "question = \"Explain this shap visualisation. What features are most important in this object?\"\n",
    "input_folder = \"images/output/\"\n",
    "\n",
    "# Ask the model with the option to display images set to True\n",
    "response = ask_model(image_name, class_id, question, input_folder, display_images_flag=True)\n",
    "\n",
    "# Print the response if it's generated\n",
    "if response:\n",
    "    print(\"Response received and processed.\")\n",
    "else:\n",
    "    print(\"No response generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fcb2d9e-f37a-46ec-b2c7-a04a45ace9ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
