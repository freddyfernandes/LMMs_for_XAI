{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Saving the visualisation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.49.0\n",
    "!pip install ultralytics\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install shap\n",
    "!pip install torch torchvision torchaudio \n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install bitsandbytes accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from ultralytics import YOLO\n",
    "from skimage import color, segmentation\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# Configure logging to log errors\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Toggle for drawing bounding boxes on final outputs\n",
    "ADD_BBOX = True\n",
    "\n",
    "# Initialize your custom-trained YOLOv8 model\n",
    "model = YOLO(\"\")\n",
    "\n",
    "# Class index to class name mapping\n",
    "class_name_mapping = {0: \"part_1\", 1: \"part_2\", 2: \"part_3\"}\n",
    "\n",
    "###############################################################################\n",
    "# 1. Modified DRISE Pipeline with Bounding Box Toggle\n",
    "###############################################################################\n",
    "def generate_drise_saliency_per_class(model, image_path, output_directory, label_names,\n",
    "                                        n_masks=10000, grid_size=(16, 16), prob_thresh=0.2,\n",
    "                                        saliency_threshold=0, draw_bbox=False, batch_size=32):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        logging.error(f\"Failed to read image: {image_path}\")\n",
    "        return\n",
    "    image_h, image_w = image.shape[:2]\n",
    "\n",
    "    # Run YOLO on the image and extract predictions\n",
    "    results = model(image)\n",
    "    preds = results[0].boxes.xyxy.cpu().numpy()\n",
    "    scores = results[0].boxes.conf.cpu().numpy()\n",
    "    pred_classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    # Get the highest-confidence box per class\n",
    "    best_boxes = {}\n",
    "    for i in range(len(preds)):\n",
    "        class_id = pred_classes[i]\n",
    "        confidence = scores[i]\n",
    "        bbox = preds[i]\n",
    "        if class_id not in best_boxes or confidence > best_boxes[class_id]['confidence']:\n",
    "            best_boxes[class_id] = {'bbox': bbox, 'confidence': confidence}\n",
    "\n",
    "    # For each detected class, generate a saliency map\n",
    "    for class_id, info in best_boxes.items():\n",
    "        if class_id >= len(label_names):\n",
    "            logging.warning(f\"Class ID {class_id} exceeds label names length. Skipping.\")\n",
    "            continue\n",
    "        target_box = info['bbox']\n",
    "        class_name = label_names[class_id]\n",
    "\n",
    "        saliency_map = np.zeros((image_h, image_w), dtype=np.float32)\n",
    "        batch_masks = []  # accumulate masks for batched inference\n",
    "\n",
    "        for i in range(n_masks):\n",
    "            mask = generate_mask((image_w, image_h), grid_size, prob_thresh)\n",
    "            batch_masks.append(mask)\n",
    "            # When we have a full batch or are at the last mask, run batched inference:\n",
    "            if len(batch_masks) == batch_size or i == n_masks - 1:\n",
    "                masked_images = [mask_image(image, m) for m in batch_masks]\n",
    "                batch_results = model.predict(source=masked_images, save=False, verbose=False)\n",
    "                for m, result in zip(batch_masks, batch_results):\n",
    "                    if len(result.boxes) > 0:\n",
    "                        masked_preds = result.boxes.xyxy.cpu().numpy()\n",
    "                        masked_scores = result.boxes.conf.cpu().numpy()\n",
    "                        masked_classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                        for box, score, cls in zip(masked_preds, masked_scores, masked_classes):\n",
    "                            if cls == class_id:\n",
    "                                iou_score = iou(target_box, box)\n",
    "                                saliency_map += m * iou_score * score\n",
    "                batch_masks = []  # clear batch\n",
    "\n",
    "        # Normalize and threshold the saliency map\n",
    "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-6)\n",
    "        saliency_map[saliency_map < saliency_threshold] = 0\n",
    "\n",
    "        heatmap = cv2.applyColorMap((saliency_map * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        cam = cv2.addWeighted(image, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "        # Draw bounding box if enabled (only the box, no text)\n",
    "        if draw_bbox:\n",
    "            x_min, y_min, x_max, y_max = map(int, target_box)\n",
    "            cv2.rectangle(cam, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        output_path = os.path.join(output_directory, f\"{image_name}_{class_name}_saliency.png\")\n",
    "        cv2.imwrite(output_path, cam)\n",
    "\n",
    "\n",
    "# Supporting functions for DRISE\n",
    "def generate_mask(image_size, grid_size, prob_thresh):\n",
    "    image_w, image_h = image_size\n",
    "    grid_w, grid_h = grid_size\n",
    "    cell_w, cell_h = math.ceil(image_w / grid_w), math.ceil(image_h / grid_h)\n",
    "    up_w, up_h = (grid_w + 1) * cell_w, (grid_h + 1) * cell_h\n",
    "    mask = (np.random.uniform(0, 1, size=(grid_h, grid_w)) < prob_thresh).astype(np.float32)\n",
    "    mask = cv2.resize(mask, (up_w, up_h), interpolation=cv2.INTER_LINEAR)\n",
    "    offset_w = np.random.randint(0, cell_w)\n",
    "    offset_h = np.random.randint(0, cell_h)\n",
    "    return mask[offset_h:offset_h + image_h, offset_w:offset_w + image_w]\n",
    "\n",
    "def mask_image(image, mask):\n",
    "    return ((image.astype(np.float32) / 255 * np.dstack([mask] * 3)) * 255).astype(np.uint8)\n",
    "\n",
    "def iou(box1, box2):\n",
    "    box1 = np.array(box1)\n",
    "    box2 = np.array(box2)\n",
    "    tl = np.maximum(box1[:2], box2[:2])\n",
    "    br = np.minimum(box1[2:], box2[2:])\n",
    "    intersection = np.prod(np.maximum(0, br - tl))\n",
    "    area1 = np.prod(box1[2:] - box1[:2])\n",
    "    area2 = np.prod(box2[2:] - box2[:2])\n",
    "    return intersection / (area1 + area2 - intersection + 1e-6)\n",
    "\n",
    "###############################################################################\n",
    "# 2. Modified SHAP Pipeline (Superpixel-based) with Bounding Box Toggle\n",
    "###############################################################################\n",
    "def run_shap_on_slic_for_classes(image_path, output_dir,\n",
    "                                 k=20, m=10, num_iter=5,\n",
    "                                 final_size=(256, 256), nsamples=50,\n",
    "                                 add_boundaries=False, draw_bbox=False):\n",
    "    ensure_dir(output_dir)\n",
    "\n",
    "    original_bgr = cv2.imread(image_path)\n",
    "    if original_bgr is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    img_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    # Run YOLO on the full-size image and save a check image with boxes\n",
    "    results = model.predict(source=[original_bgr], save=False, verbose=False, imgsz=512)\n",
    "    result = results[0]\n",
    "    boxed_image = draw_yolo_boxes(original_bgr, result, model)\n",
    "    bbox_save_path = os.path.join(output_dir, f\"{img_name}_yolo_bboxes.jpg\")\n",
    "    cv2.imwrite(bbox_save_path, boxed_image)\n",
    "    print(f\"[INFO] Saved YOLO bbox image => {bbox_save_path}\")\n",
    "\n",
    "    if len(result.boxes) == 0:\n",
    "        print(\"[INFO] No objects detected in the image.\")\n",
    "        return\n",
    "\n",
    "    # Determine the top 3 classes by highest confidence\n",
    "    cls_array = result.boxes.data[:, 5].cpu().numpy()\n",
    "    conf_array = result.boxes.data[:, 4].cpu().numpy()\n",
    "    unique_classes = np.unique(cls_array)\n",
    "    class_conf_map = {int(uc): conf_array[cls_array == uc].max() for uc in unique_classes}\n",
    "    sorted_by_conf = sorted(class_conf_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    top3_classes = [cls for cls, _ in sorted_by_conf[:3]]\n",
    "    print(f\"[INFO] Top 3 classes by confidence => {top3_classes}\")\n",
    "\n",
    "    # Resize image for SLIC and SHAP explanation\n",
    "    resized_bgr = cv2.resize(original_bgr, final_size)\n",
    "    resized_rgb = cv2.cvtColor(resized_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_lab = color.rgb2lab(resized_rgb)\n",
    "\n",
    "    # Run SLIC segmentation to obtain superpixels\n",
    "    clusters, label_map = slic_segmentation(img_lab, k=k, m=m, num_iter=num_iter)\n",
    "    num_superpixels = len(clusters)\n",
    "    print(f\"[INFO] Found {num_superpixels} superpixels with k={k}, m={m}, num_iter={num_iter}.\")\n",
    "\n",
    "    # For each of the top 3 classes, run SHAP explanation\n",
    "    for target_class in top3_classes:\n",
    "        class_name = model.names[target_class]\n",
    "        print(f\"[SHAP] Generating explanation for class: {class_name} (ID={target_class})\")\n",
    "\n",
    "        background = np.zeros((1, num_superpixels))\n",
    "        explainer = shap.KernelExplainer(\n",
    "            lambda mask_matrix: superpixel_shap_predict_for_class(mask_matrix, resized_bgr, label_map, target_class),\n",
    "            background\n",
    "        )\n",
    "        test_sample = np.ones((1, num_superpixels))\n",
    "        shap_values = explainer.shap_values(test_sample, nsamples=nsamples)\n",
    "        sv = shap_values[0][0] if isinstance(shap_values, list) else shap_values[0]\n",
    "\n",
    "        overlay = np.zeros(label_map.shape, dtype=float)\n",
    "        for sp_index in range(num_superpixels):\n",
    "            overlay[label_map == sp_index] = sv[sp_index]\n",
    "\n",
    "        # Create a grayscale background for blending\n",
    "        resized_gray = cv2.cvtColor(resized_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        resized_gray_rgb = cv2.cvtColor(resized_gray, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        cmap = cm.colormaps['bwr'] if hasattr(cm, 'colormaps') else cm.get_cmap('bwr')\n",
    "        norm = Normalize(vmin=overlay.min(), vmax=overlay.max())\n",
    "        overlay_rgba = cmap(norm(overlay))\n",
    "        threshold = 0.1 * (overlay.max() - overlay.min())\n",
    "        low_contribution_mask = np.abs(overlay) < threshold\n",
    "        overlay_rgb = (overlay_rgba[..., :3] * 255).astype(np.uint8)\n",
    "        alpha = 0.5\n",
    "\n",
    "        blended = resized_gray_rgb.copy()\n",
    "        contributing_mask = ~low_contribution_mask\n",
    "        for c in range(3):\n",
    "            blended[..., c] = np.where(\n",
    "                contributing_mask,\n",
    "                (1.0 - alpha) * resized_gray_rgb[..., c] + alpha * overlay_rgb[..., c],\n",
    "                resized_gray_rgb[..., c],\n",
    "            )\n",
    "\n",
    "        if add_boundaries:\n",
    "            boundaries = segmentation.find_boundaries(label_map, mode='outer')\n",
    "            blended[boundaries] = [0, 0, 0]\n",
    "\n",
    "        # If toggled, draw the bounding box for this target class on the blended image.\n",
    "        if draw_bbox:\n",
    "            best_box = None\n",
    "            best_conf = -1\n",
    "            for b in result.boxes.data:\n",
    "                b_cpu = b.cpu().numpy()\n",
    "                x1, y1, x2, y2, conf, cls_id = b_cpu\n",
    "                if int(cls_id) == target_class and conf > best_conf:\n",
    "                    best_conf = conf\n",
    "                    best_box = (x1, y1, x2, y2)\n",
    "            if best_box is not None:\n",
    "                scale_x = final_size[0] / original_bgr.shape[1]\n",
    "                scale_y = final_size[1] / original_bgr.shape[0]\n",
    "                scaled_box = (int(best_box[0] * scale_x), int(best_box[1] * scale_y),\n",
    "                              int(best_box[2] * scale_x), int(best_box[3] * scale_y))\n",
    "                cv2.rectangle(blended, (scaled_box[0], scaled_box[1]), (scaled_box[2], scaled_box[3]),\n",
    "                              (0, 255, 0), 2)\n",
    "                cv2.putText(blended, f\"{class_name} {best_conf:.2f}\",\n",
    "                            (scaled_box[0], scaled_box[1] - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"{img_name}_{k}_{num_iter}_{nsamples}_{class_name}_shap.jpg\")\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(blended, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"[SHAP] Saved superpixel explanation => {output_path}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Additional supporting functions for the superpixel SHAP pipeline\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def draw_yolo_boxes(image_bgr, result, model):\n",
    "    draw_img = image_bgr.copy()\n",
    "    if len(result.boxes) == 0:\n",
    "        return draw_img\n",
    "    boxes_data = result.boxes.data.cpu().numpy()\n",
    "    classes = boxes_data[:, 5].astype(int)\n",
    "    confidences = boxes_data[:, 4]\n",
    "    best_boxes = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        if cls not in best_boxes or confidences[i] > best_boxes[cls][4]:\n",
    "            best_boxes[cls] = boxes_data[i]\n",
    "    for cls, box in best_boxes.items():\n",
    "        x1, y1, x2, y2, conf, cls_id = box\n",
    "        class_name = model.names[int(cls_id)]\n",
    "        cv2.rectangle(draw_img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        text = f\"{class_name} {conf:.2f}\"\n",
    "        cv2.putText(draw_img, text, (int(x1), int(y1) - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    return draw_img\n",
    "\n",
    "def superpixel_shap_predict_for_class(mask_matrix, original_image_bgr, label_map, target_class):\n",
    "    batch_size = mask_matrix.shape[0]\n",
    "    images = []\n",
    "    for i in range(batch_size):\n",
    "        mask = mask_matrix[i]\n",
    "        masked_img = mask_superpixels(original_image_bgr, label_map, mask)\n",
    "        images.append(masked_img)\n",
    "    confs = yolo_predict_for_class(images, target_class)\n",
    "    return confs\n",
    "\n",
    "def yolo_predict_for_class(images_bgr, target_class):\n",
    "    results = model.predict(source=images_bgr, save=False, verbose=False, imgsz=512)\n",
    "    confidences = []\n",
    "    for result in results:\n",
    "        if len(result.boxes) > 0:\n",
    "            cls_array = result.boxes.data[:, 5].cpu().numpy()\n",
    "            conf_array = result.boxes.data[:, 4].cpu().numpy()\n",
    "            filtered_indices = (cls_array == target_class)\n",
    "            filtered_boxes = conf_array[filtered_indices]\n",
    "            conf = float(filtered_boxes.max()) if len(filtered_boxes) > 0 else 0.0\n",
    "        else:\n",
    "            conf = 0.0\n",
    "        confidences.append(conf)\n",
    "    return np.array(confidences)\n",
    "\n",
    "def mask_superpixels(image_bgr, label_map, mask):\n",
    "    masked_img = np.zeros_like(image_bgr, dtype=np.uint8)\n",
    "    num_superpixels = mask.shape[0]\n",
    "    for sp_index in range(num_superpixels):\n",
    "        if mask[sp_index] == 1:\n",
    "            masked_img[label_map == sp_index] = image_bgr[label_map == sp_index]\n",
    "    return masked_img\n",
    "\n",
    "def slic_segmentation(img_lab, k=100, m=20, num_iter=10):\n",
    "    img_h, img_w = img_lab.shape[:2]\n",
    "    N = img_h * img_w\n",
    "    S = int(math.sqrt(N / k))\n",
    "    clusters = []\n",
    "    tag = {}\n",
    "    dis = np.full((img_h, img_w), np.inf)\n",
    "    clusters = initial_cluster_center(S, img_lab, img_h, img_w, clusters)\n",
    "    reassign_cluster_center_acc_to_grad(clusters, img_lab, img_h, img_w)\n",
    "    for _ in range(num_iter):\n",
    "        assign_pixels_to_cluster(clusters, S, img_lab, img_h, img_w, tag, dis, m)\n",
    "        update_cluster_mean(clusters, img_lab)\n",
    "    label_map = np.zeros((img_h, img_w), dtype=np.int32)\n",
    "    for idx, c in enumerate(clusters):\n",
    "        for (ph, pw) in c.pixels:\n",
    "            label_map[ph, pw] = idx\n",
    "    return clusters, label_map\n",
    "\n",
    "def initial_cluster_center(S, img_lab, img_h, img_w, clusters):\n",
    "    h = S // 2\n",
    "    w_init = S // 2\n",
    "    while h < img_h:\n",
    "        w = w_init\n",
    "        while w < img_w:\n",
    "            clusters.append(make_superPixel(h, w, img_lab))\n",
    "            w += S\n",
    "        h += S\n",
    "    return clusters\n",
    "\n",
    "def make_superPixel(h, w, img_lab):\n",
    "    return SuperPixels(h, w, img_lab[h, w, 0], img_lab[h, w, 1], img_lab[h, w, 2])\n",
    "\n",
    "class SuperPixels(object):\n",
    "    def __init__(self, h, w, l=0, a=0, b=0):\n",
    "        self.update(h, w, l, a, b)\n",
    "        self.pixels = []\n",
    "    def update(self, h, w, l, a, b):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.l = l\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "def calc_gradient(h, w, img_lab, img_w, img_h):\n",
    "    if w + 1 >= img_w:\n",
    "        w = img_w - 2\n",
    "    if h + 1 >= img_h:\n",
    "        h = img_h - 2\n",
    "    grad = ((img_lab[h + 1, w + 1, 0] - img_lab[h, w, 0]) +\n",
    "            (img_lab[h + 1, w + 1, 1] - img_lab[h, w, 1]) +\n",
    "            (img_lab[h + 1, w + 1, 2] - img_lab[h, w, 2]))\n",
    "    return abs(grad)\n",
    "\n",
    "def reassign_cluster_center_acc_to_grad(clusters, img_lab, img_h, img_w):\n",
    "    for c in clusters:\n",
    "        base_grad = calc_gradient(c.h, c.w, img_lab, img_w, img_h)\n",
    "        best_h, best_w = c.h, c.w\n",
    "        best_grad = base_grad\n",
    "        for dh in range(-1, 2):\n",
    "            for dw in range(-1, 2):\n",
    "                H = c.h + dh\n",
    "                W = c.w + dw\n",
    "                if 0 <= H < img_h and 0 <= W < img_w:\n",
    "                    new_grad = calc_gradient(H, W, img_lab, img_w, img_h)\n",
    "                    if new_grad < best_grad:\n",
    "                        best_h, best_w = H, W\n",
    "                        best_grad = new_grad\n",
    "        if (best_h, best_w) != (c.h, c.w):\n",
    "            c.update(best_h, best_w,\n",
    "                     img_lab[best_h, best_w, 0],\n",
    "                     img_lab[best_h, best_w, 1],\n",
    "                     img_lab[best_h, best_w, 2])\n",
    "\n",
    "def assign_pixels_to_cluster(clusters, S, img_lab, img_h, img_w, tag, dis, m):\n",
    "    for c in clusters:\n",
    "        row_start = max(c.h - 2 * S, 0)\n",
    "        row_end = min(c.h + 2 * S, img_h)\n",
    "        col_start = max(c.w - 2 * S, 0)\n",
    "        col_end = min(c.w + 2 * S, img_w)\n",
    "        for h in range(row_start, row_end):\n",
    "            for w in range(col_start, col_end):\n",
    "                L, A, B = img_lab[h, w]\n",
    "                Dc = math.sqrt((L - c.l) ** 2 + (A - c.a) ** 2 + (B - c.b) ** 2)\n",
    "                Ds = math.sqrt((h - c.h) ** 2 + (w - c.w) ** 2)\n",
    "                D = math.sqrt((Dc / m) ** 2 + (Ds / S) ** 2)\n",
    "                if D < dis[h, w]:\n",
    "                    if (h, w) in tag:\n",
    "                        old_cluster = tag[(h, w)]\n",
    "                        if (h, w) in old_cluster.pixels:\n",
    "                            old_cluster.pixels.remove((h, w))\n",
    "                    tag[(h, w)] = c\n",
    "                    c.pixels.append((h, w))\n",
    "                    dis[h, w] = D\n",
    "\n",
    "def update_cluster_mean(clusters, img_lab):\n",
    "    for c in clusters:\n",
    "        if len(c.pixels) == 0:\n",
    "            continue\n",
    "        sum_h = 0\n",
    "        sum_w = 0\n",
    "        sum_L = 0.0\n",
    "        sum_A = 0.0\n",
    "        sum_B = 0.0\n",
    "        for (ph, pw) in c.pixels:\n",
    "            sum_h += ph\n",
    "            sum_w += pw\n",
    "            sum_L += img_lab[ph, pw, 0]\n",
    "            sum_A += img_lab[ph, pw, 1]\n",
    "            sum_B += img_lab[ph, pw, 2]\n",
    "        count = len(c.pixels)\n",
    "        mean_h = sum_h // count\n",
    "        mean_w = sum_w // count\n",
    "        mean_L = sum_L / count\n",
    "        mean_A = sum_A / count\n",
    "        mean_B = sum_B / count\n",
    "        c.update(mean_h, mean_w, mean_L, mean_A, mean_B)\n",
    "\n",
    "###############################################################################\n",
    "# 3. Main Loop: Process all images using the modified pipelines\n",
    "###############################################################################\n",
    "\n",
    "input_dir = ''\n",
    "output_dir = \"\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process images with a tqdm progress bar and error logging\n",
    "for image_name in tqdm(sorted(os.listdir(input_dir)), desc=\"Processing images\"):\n",
    "    image_path = os.path.join(input_dir, image_name)\n",
    "    if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        try:\n",
    "            generate_drise_saliency_per_class(model, image_path, output_dir, \n",
    "                                              list(class_name_mapping.values()),\n",
    "                                              n_masks=1000, saliency_threshold=0, \n",
    "                                              draw_bbox=ADD_BBOX)\n",
    "\n",
    "            run_shap_on_slic_for_classes(image_path, output_dir,\n",
    "                                         final_size=(512, 512), nsamples=100,\n",
    "                                         k=40, m=10, num_iter=20,\n",
    "                                         add_boundaries=False, draw_bbox=ADD_BBOX)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "print(\"DRISE and SHAP for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Large Multimodal Model Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 1. Logging Setup\n",
    "# --------------------------------------------------\n",
    "# Suppress INFO-level logs from accelerate and transformers\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "# Optionally, set environment variables for transformers verbosity:\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Global Config & Model Load\n",
    "# --------------------------------------------------\n",
    "DEVICE = \"cuda:0\"  # or just \"cuda\" if you have one GPU\n",
    "model_name = \"HuggingFaceM4/Idefics2-8b\"  # Make sure you have access to this model\n",
    "\n",
    "# Standard default image path (adjust as needed)\n",
    "DEFAULT_IMAGE_PATH = \"\"\n",
    "\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",  # remove if you get errors\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"OutOfMemoryError while loading the model. Attempting to free cache...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    raise\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Prompt & Generation Functions\n",
    "# --------------------------------------------------\n",
    "def generate_prompt_for_eval(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Builds a system+user prompt with a delimiter (\"### Answer:\") appended\n",
    "    so that only the answer portion is extracted from the generation.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"\n",
    "\n",
    "    Context:\n",
    "    This image is the Saliency Map Visualization which emphasizes the areas of the image that the model finds most critical for its decision-making process.\n",
    "    By highlighting the most 'important' pixels, the saliency map allows us to see which regions of the image had the most influence on the model predictions.\n",
    "    It visually demonstrates how the model identifies and delineates the target object in relation to the surrounding elements.\n",
    "    Instructions for the Response:\n",
    "     - Focus on what the saliency map reveals about the object's location or features, without referencing any color details.\n",
    "     - Provide a clear and simple explanation.\n",
    "     - Use straightforward language to describe how the visualization contributes to understanding the detection.\n",
    "     - Limit the output to 10-20 words.\n",
    "  \n",
    "    ### Answer:\n",
    "    \"\"\"\n",
    "    user_text = question if question else \"Hello!\"\n",
    "    \n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"sytem\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_prompt.strip()}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": user_text}]\n",
    "        }\n",
    "    ]\n",
    "    return prompt\n",
    "\n",
    "def ask_model_eval(question: str, image_path: str = DEFAULT_IMAGE_PATH):\n",
    "    \"\"\"\n",
    "    1) Builds a prompt (using a default image path).\n",
    "    2) Passes it to the model for generation.\n",
    "    3) Extracts and returns only the answer after the delimiter.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build chat-like prompt with delimiter\n",
    "        messages = generate_prompt_for_eval(question)\n",
    "        \n",
    "        # If the default image exists, insert it into the prompt\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            messages[1][\"content\"].insert(0, {\"type\": \"image\", \"image\": Image.open(image_path)})\n",
    "\n",
    "        # Apply chat template logic to build the prompt text\n",
    "        prompt_text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "\n",
    "        # Prepare inputs, including image if available\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            inputs = processor(\n",
    "                text=prompt_text,\n",
    "                images=[Image.open(image_path)],\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "        else:\n",
    "            inputs = processor(\n",
    "                text=prompt_text,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "        # Generate output\n",
    "        pixel_values = inputs.get(\"pixel_values\", None)\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=pixel_values,  # None if no image\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        if generated_ids is None or len(generated_ids) == 0:\n",
    "            print(\"No output generated.\")\n",
    "            return None\n",
    "\n",
    "        output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        # Extract only the answer after the delimiter \"### Answer:\"\n",
    "        if \"### Answer:\" in output_text:\n",
    "            final_reply = output_text.split(\"### Answer:\")[-1].strip()\n",
    "        else:\n",
    "            final_reply = output_text.strip()\n",
    "\n",
    "        if not final_reply:\n",
    "            return None\n",
    "\n",
    "        return final_reply\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError as oom_e:\n",
    "        print(\"CUDA OutOfMemoryError encountered during generation!\")\n",
    "        torch.cuda.empty_cache()\n",
    "        raise oom_e\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ask_model_eval: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Widgets for Interactive Demo\n",
    "# --------------------------------------------------\n",
    "\n",
    "# (A) Text Input for the question (only question input now)\n",
    "question_input = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Type your question here\",\n",
    "    description=\"Question:\",\n",
    "    layout=widgets.Layout(width=\"90%\")\n",
    ")\n",
    "\n",
    "# (B) Button to trigger generation\n",
    "eval_button = widgets.Button(\n",
    "    description=\"Get Model Answer\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "\n",
    "# (C) Output area\n",
    "out = widgets.Output()\n",
    "\n",
    "# Button callback\n",
    "def on_button_clicked(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        question = question_input.value\n",
    "\n",
    "        print(f\"\\n--- Generating answer for question: '{question}' ---\")\n",
    "        response = ask_model_eval(question)  # Uses default image path\n",
    "\n",
    "        print(\"\\nModel Answer:\")\n",
    "        if response:\n",
    "            print(response)\n",
    "        else:\n",
    "            print(\"No valid response was generated. Check for OOM or model issues.\")\n",
    "\n",
    "eval_button.on_click(on_button_clicked)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Display the Widget\n",
    "# --------------------------------------------------\n",
    "display(question_input, eval_button, out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
